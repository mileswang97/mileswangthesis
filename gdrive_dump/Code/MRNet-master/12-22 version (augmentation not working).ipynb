{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"12-22 version (augmentation not working).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"7RQQrFFtr1kC","colab_type":"code","outputId":"d739fa64-c08f-4634-b248-4b51113d0ac4","executionInfo":{"status":"ok","timestamp":1577376595649,"user_tz":300,"elapsed":27917,"user":{"displayName":"Miles Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCp1D9GSnniuak1b4qTc8LyXIYWr5-w3JOCol8uMDoJ9DjR6Oc2jUCzv2CLHZySNgZQZKBOu9XQQ61wZgOFmS7HnfhhjOjtGNycWsDYEOVN2QXaIyeAFjqs1wpthUa5qoxxSxIBIQwsQRbqshUrkEXMM12r56GOEq2njIfcq6fpRLzNSIP4KBJWU_kv2U3nBqRgN_Z4BKSs6VZeo9k7SOcT1wnSyavpy59kXDXuZkE4b1xBPgAqftswGN9ptbGevL8XW6FnD2y4RElZw5Rn4lZEKAE7zuZfZFtc9EdjF55pCSmgzgG8K6e6w1pPSO-vbRb62xAFhsWuT_CrDp70jm2kPPAZTWJqJfTnfOVimQza6UzgAJeliOI9IVJvVBPxJdoTjyAsfvD-wt949h1dKESssk_kq9PgCC3wWpc8P5-UJt7W_umXtj9lk0H6oIgS9h6UhEmyNSmDc0hPnaShr8GiwNwGujjlRhoxG_UTbqbOcPUxNnHMtBvz0q4jSaATG2_scmW3KHGrm9RrE4xttEX0eee1Q-0FPvkLM5oTyBniAeZmmNZZ1JybeEfGFAmD3Zvq4tFXvDuENVKVRRdU3AZx2pt6fUh1zApSbDly5AfTvPHqX3Wfr9e7aCqo2ZKEmEWpS0um7w0uqInV8LtF1_i33iUOMdxkIdh_HpVT9-g8u5buBGiWLvbx2PJ4YcOW9hVZvIABh0mFZUTeVg9zIFHWYa037siwMhH2MExVM6MKkYfepaH5utn5JhaVbdw=s64","userId":"12217843719772555429"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import drive \n","drive.mount('/content/gdrive') \n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7jqvUXbEDHe0","colab_type":"code","colab":{}},"source":["# model.py\n","\n","import torch\n","import torch.nn as nn\n","\n","from torchvision import models\n","\n","class MRNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.model = models.alexnet(pretrained=True)\n","        #self.model = models.googlenet(pretrained=True)\n","        #self.model = models.densenet161(pretrained=True)\n","        #self.model = models.densenet121(pretrained=True)\n","        self.gap = nn.AdaptiveAvgPool2d(1)\n","        self.classifier = nn.Linear(256, 1)\n","\n","    def forward(self, x):\n","        x = torch.squeeze(x, dim=0) # only batch size 1 supported\n","        x = self.model.features(x)\n","        x = self.gap(x).view(x.size(0), -1)\n","        x = torch.max(x, 0, keepdim=True)[0]\n","        x = self.classifier(x)\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dNtBJxgjDWSi","colab_type":"code","outputId":"b2dc43f3-47ea-468e-ccd1-abf4e939a058","executionInfo":{"status":"ok","timestamp":1577376573664,"user_tz":300,"elapsed":7788,"user":{"displayName":"Miles Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCp1D9GSnniuak1b4qTc8LyXIYWr5-w3JOCol8uMDoJ9DjR6Oc2jUCzv2CLHZySNgZQZKBOu9XQQ61wZgOFmS7HnfhhjOjtGNycWsDYEOVN2QXaIyeAFjqs1wpthUa5qoxxSxIBIQwsQRbqshUrkEXMM12r56GOEq2njIfcq6fpRLzNSIP4KBJWU_kv2U3nBqRgN_Z4BKSs6VZeo9k7SOcT1wnSyavpy59kXDXuZkE4b1xBPgAqftswGN9ptbGevL8XW6FnD2y4RElZw5Rn4lZEKAE7zuZfZFtc9EdjF55pCSmgzgG8K6e6w1pPSO-vbRb62xAFhsWuT_CrDp70jm2kPPAZTWJqJfTnfOVimQza6UzgAJeliOI9IVJvVBPxJdoTjyAsfvD-wt949h1dKESssk_kq9PgCC3wWpc8P5-UJt7W_umXtj9lk0H6oIgS9h6UhEmyNSmDc0hPnaShr8GiwNwGujjlRhoxG_UTbqbOcPUxNnHMtBvz0q4jSaATG2_scmW3KHGrm9RrE4xttEX0eee1Q-0FPvkLM5oTyBniAeZmmNZZ1JybeEfGFAmD3Zvq4tFXvDuENVKVRRdU3AZx2pt6fUh1zApSbDly5AfTvPHqX3Wfr9e7aCqo2ZKEmEWpS0um7w0uqInV8LtF1_i33iUOMdxkIdh_HpVT9-g8u5buBGiWLvbx2PJ4YcOW9hVZvIABh0mFZUTeVg9zIFHWYa037siwMhH2MExVM6MKkYfepaH5utn5JhaVbdw=s64","userId":"12217843719772555429"}},"colab":{"base_uri":"https://localhost:8080/","height":252}},"source":["# loader.py\n","\n","!pip install medicaltorch\n","\n","import numpy as np\n","import os\n","import pickle\n","import torch\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","import torchvision\n","from medicaltorch import transforms as mt_transforms\n","import PIL\n","from random import sample\n","\n","from torch.autograd import Variable\n","\n","INPUT_DIM = 224\n","MAX_PIXEL_VAL = 255\n","MEAN = 58.09\n","STDDEV = 49.73\n","\n","class Dataset(data.Dataset):\n","    def __init__(self, datadirs, diagnosis, use_gpu, transformbool):\n","        super().__init__()\n","        self.use_gpu = use_gpu\n","\n","        self.transformbool = transformbool\n","        self.transforms = torchvision.transforms.Compose([\n","        #    torchvision.transforms.ToPILImage(),\n","            #torchvision.transforms.Resize((224,224)),\n","            #torchvision.transforms.ColorJitter(hue=.05, saturation=.05),\n","        #    torchvision.transforms.RandomHorizontalFlip(p=1.0),\n","        #    torchvision.transforms.RandomRotation((20,20), resample=PIL.Image.BILINEAR),\n","            #torchvision.transforms.RandomCrop((224,224),pad_if_needed=True),\n","            #torchvision.transforms.Resize((224,224)),\n","        #    torchvision.transforms.ToTensor()\n","        \n","            mt_transforms.CenterCrop2D((200,200)),\n","            mt_transforms.RandomAffine(degrees = 0.0, scale = (1, 1), translate=(0,0)),\n","            mt_transforms.ToTensor()\n","\n","         ])\n","        \n","        self.train_transforms = torchvision.transforms.Compose([\n","            mt_transforms.Resample(0.25, 0.25),\n","            mt_transforms.ElasticTransform(alpha_range=(40.0, 60.0),\n","                                           sigma_range=(2.5, 4.0),\n","                                           p=0.3),\n","            mt_transforms.ToTensor()]\n","        )\n","        \n","        label_dict = {}\n","        self.paths = []\n","        print(datadirs)\n","\n","        for i, line in enumerate(open('metadata.csv').readlines()):\n","            if i == 0:\n","                continue\n","            line = line.strip().split(',')\n","            path = line[10]\n","            label = line[2]\n","            label_dict[path] = int(int(label) > diagnosis)\n","\n","        for dir in datadirs:\n","            for file in os.listdir(dir):\n","                self.paths.append(dir+'/'+file)\n","\n","        self.labels = [label_dict[path[6:]] for path in self.paths]\n","\n","        neg_weight = np.mean(self.labels)\n","        self.weights = [neg_weight, 1 - neg_weight]\n","\n","    def weighted_loss(self, prediction, target):\n","        weights_npy = np.array([self.weights[int(t[0])] for t in target.data])\n","        weights_tensor = torch.FloatTensor(weights_npy)\n","        if self.use_gpu:\n","            weights_tensor = weights_tensor.cuda()\n","        loss = F.binary_cross_entropy_with_logits(prediction, target, weight=Variable(weights_tensor))\n","        return loss\n","\n","      \n","    # Data augmentation section\n","    # can go through each cases, looking at the histogram of 3T vs 1.5T (naive distribution of contrast data?)\n","    def __getitem__(self, index):\n","        path = self.paths[index]\n","        with open(path, 'rb') as file_handler: # Must use 'rb' as the data is binary\n","            vol = pickle.load(file_handler).astype(np.int32)\n","        \n","        # crop middle\n","        pad = int((vol.shape[2] - INPUT_DIM)/2)\n","        #print('pad', pad)\n","        vol = vol[:,pad:-pad,pad:-pad]\n","        #vol = vol[pad:-pad,pad:-pad,:]\n","  \n","        # see if theres a way to reformat an image from 196 to 224 \n","        # something called interpolate, scikit image. \n","        # consider scipy zoom too?\n","        problemflag = False\n","\n","        if not(vol.shape[1] == 224) or not(vol.shape[2] == 224):\n","          #print('problem vol shape', vol.shape)\n","          delta_1 = (INPUT_DIM - vol.shape[1]) // 2\n","          delta_2 = (INPUT_DIM - vol.shape[2]) // 2\n","          padding = (delta_1, delta_2)\n","          new_vol = np.zeros((vol.shape[0], 224, 224), dtype=np.int32)\n","          for slice in range(vol.shape[0]):\n","            vol_slice = vol[slice,:,:]\n","            img_slice = PIL.Image.fromarray(vol_slice)\n","            new_vol[slice,:,:] = np.array(PIL.ImageOps.fit(img_slice, [224, 224]), dtype='i')\n","          vol = new_vol  \n","          vol.astype(np.int32)\n","          problemflag = True\n","          #print('vol shape', vol.shape)\n","          #print('vol type', vol.dtype)\n","        flag = False\n","        randomangle = 0\n","        if self.transformbool:\n","          if np.random.rand(1) < 0.5:\n","            flag = True\n","            randomangle = np.random.uniform(-20,20)\n","            self.transforms = torchvision.transforms.Compose([\n","              torchvision.transforms.ToPILImage(),\n","              #torchvision.transforms.Resize((224,224)),\n","              torchvision.transforms.RandomHorizontalFlip(p=1.0),\n","              torchvision.transforms.RandomRotation((randomangle,randomangle), resample=PIL.Image.BILINEAR),\n","              #torchvision.transforms.RandomCrop((224,224),pad_if_needed=True),\n","              torchvision.transforms.ToTensor()\n","          ])\n","\n","        \"\"\"\n","        # see if this transform policy can take in a 3d volume\n","        #vol = np.asarray(vol).astype(np.uint8)\n","        # save the 15th slice to see if transformation happened\n","        #print('volume mean', np.mean(vol, axis=0))\n","        #save_fig = PIL.Image.fromarray(np.uint8(np.array(vol[15,:,:] * 255)))\n","        save_fig = PIL.Image.fromarray(np.uint8(np.array(vol[15,:,:])))\n","        save_fig = save_fig.convert(\"L\")\n","        newpath = str(path).replace('/', ' ')\n","        file_name = \"path\" + newpath + \" transform\" + str(flag) + \" angle\" + str(round(randomangle)) + \"_0.png\"\n","        save_path = Path(rundir) / \"images1\" /  file_name\n","        save_fig.save(save_path)\n","        \"\"\"\n","\n","        #vol = vol.transpose(2,1,0)\n","        #print('vol shape', vol.shape)\n","        #print('vol sum', np.sum(vol))\n","\n","        #print('vol type', vol.dtype)\n","        if flag:\n","          for sliceindex in range(vol.shape[0]):\n","            vol[sliceindex] = self.transforms(np.array(vol[sliceindex]))\n","\n","\n","        # standardize\n","        vol = (vol - np.min(vol)) / (np.max(vol) - np.min(vol) + 1.0e-6) * MAX_PIXEL_VAL\n","\n","        save_fig = PIL.Image.fromarray(np.uint8(np.array(vol[15,:,:])))\n","        save_fig = save_fig.convert(\"L\") \n","        #save_fig.show()\n","        #file_name = f'path{path}_transform{flag}_angle{randomangle:0.4f}'\n","        newpath = str(path).replace('/', ' ')\n","        file_name = \"path\" + newpath + \" transform\" + str(flag) + \" angle\" + str(round(randomangle)) + \"_1.png\"\n","        save_path = Path(rundir) / \"images2\" /  file_name\n","        save_fig.save(save_path)\n","\n","        if problemflag:\n","          save_fig = PIL.Image.fromarray(np.uint8(np.array(vol[15,:,:])))\n","          save_fig = save_fig.convert(\"L\") \n","          #save_fig.show()\n","          #file_name = f'path{path}_transform{flag}_angle{randomangle:0.4f}'\n","          newpath = str(path).replace('/', ' ')\n","          file_name = \"path\" + newpath + \" transform\" + str(flag) + \" angle\" + str(round(randomangle)) + \"_1.png\"\n","          save_path = Path(rundir) / \"problems\" /  file_name\n","          save_fig.save(save_path)\n","\n","        # normalize\n","        vol = (vol - MEAN) / STDDEV\n","        #print('vol1', vol.shape)\n","\n","        #print('vol shape', vol.shape)\n","\n","        # convert to RGB\n","        #vol = np.stack((vol,)*3, axis=1)\n","        #print('vol2', vol.shape)\n","\n","        #new_vol = self.transforms(vol).float()\n","        #print('new_vol', new_vol.shape)\n","\n","        #assert(1==2)\n","        #if self.transformbool:\n","        #  vol = np.mean(self.transforms(vol).float(), axis=(0,1))\n","\n","        \n","        save_fig = PIL.Image.fromarray(np.uint8(np.array(vol[15,:,:])))\n","        save_fig = save_fig.convert(\"L\")\n","        #save_fig.show()\n","        #file_name = f'path{path}_transform{flag}_angle{randomangle:0.4f}'\n","        newpath = str(path).replace('/', ' ')\n","        file_name = \"path\" + newpath + \" transform\" + str(flag) + \" angle\" + str(round(randomangle)) + \"_2.png\"\n","        save_path = Path(rundir) / \"images3\" /  file_name\n","        save_fig.save(save_path)\n","        \n","\n","        vol = np.stack((vol,)*3, axis=1)\n","\n","        vol_tensor = torch.FloatTensor(vol)\n","        label_tensor = torch.FloatTensor([self.labels[index]])\n","        #print('vol tensor shape', vol_tensor.shape)\n","        #print('label_tensor shape', label_tensor.shape)\n","        assert(1==3)\n","        return vol_tensor, label_tensor\n","\n","    def __len__(self):\n","        return len(self.paths)\n","\n","def load_data(self, diagnosis, use_gpu=True):\n","\n","    #vol_list = ['vol01', 'vol02', 'vol03', 'vol04', 'vol05', 'vol06', 'vol07', 'vol08', 'vol09', 'vol10']\n","    #vol_ind = [0,1,2,3,4,5,6,7,8,9]\n","    \n","    #train_ind = sample(vol_ind, 6)\n","    #train_dirs = [vol_list[i] for i in train_ind]\n","\n","    #val_test_ind = [index for index in vol_ind if index not in train_ind]\n","    #val_ind = sample(val_test_ind, 2)\n","    #valid_dirs = [vol_list[i] for i in val_ind]\n","\n","    #test_ind = [index for index in val_test_ind if index not in val_ind]\n","    #test_dirs = [vol_list[i] for i in test_ind]\n","    train_dirs = ['vol08','vol04','vol03','vol09','vol06','vol07']\n","    valid_dirs = ['vol10','vol05']\n","    test_dirs = ['vol01','vol02']\n","    \n","    #train_dataset = Dataset(train_dirs, diagnosis, use_gpu)\n","    train_dataset = Dataset(train_dirs, diagnosis, use_gpu, True)\n","    valid_dataset = Dataset(valid_dirs, diagnosis, use_gpu, False)\n","    test_dataset = Dataset(test_dirs, diagnosis, use_gpu, False)\n","\n","    train_loader = data.DataLoader(train_dataset, batch_size=1, num_workers=8, shuffle=True)\n","    valid_loader = data.DataLoader(valid_dataset, batch_size=1, num_workers=8, shuffle=False)\n","    test_loader = data.DataLoader(test_dataset, batch_size=1, num_workers=8, shuffle=False)\n","\n","    return train_loader, valid_loader, test_loader\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting medicaltorch\n","  Downloading https://files.pythonhosted.org/packages/5a/85/b3fa267c6cdec058c937a5046e357de61dda938179aeeca72485f13b1fa2/medicaltorch-0.2-py3-none-any.whl\n","Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.6/dist-packages (from medicaltorch) (1.17.4)\n","Requirement already satisfied: tqdm>=4.23.0 in /usr/local/lib/python3.6/dist-packages (from medicaltorch) (4.28.1)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from medicaltorch) (1.3.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from medicaltorch) (1.3.3)\n","Requirement already satisfied: nibabel>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from medicaltorch) (2.3.3)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from medicaltorch) (0.4.2)\n","Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.2.1->medicaltorch) (1.12.0)\n","Requirement already satisfied: bz2file in /usr/local/lib/python3.6/dist-packages (from nibabel>=2.2.1->medicaltorch) (0.98)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision>=0.2.1->medicaltorch) (4.3.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.1.1->torchvision>=0.2.1->medicaltorch) (0.46)\n","Installing collected packages: medicaltorch\n","Successfully installed medicaltorch-0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M_Olw5OZDqFc","colab_type":"code","colab":{}},"source":["\n","\n","# evaluate.py\n","\n","import argparse\n","import matplotlib.pyplot as plt\n","import os\n","import numpy as np\n","import torch\n","\n","from sklearn import metrics\n","from torch.autograd import Variable\n","\n","#from loader import load_data\n","#from model import MRNet\n","\n","def get_parser():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--model_path', type=str, required=True)\n","    parser.add_argument('--split', type=str, required=True)\n","    parser.add_argument('--diagnosis', type=int, required=True)\n","    parser.add_argument('--gpu', action='store_true')\n","    return parser\n","\n","def run_model(model, loader, train=False, optimizer=None):\n","    preds = []\n","    labels = []\n","\n","    if train:\n","        model.train()\n","    else:\n","        model.eval()\n","\n","    total_loss = 0.\n","    num_batches = 0\n","\n","    for batch in loader:\n","        if train:\n","            optimizer.zero_grad()\n","\n","        vol, label = batch\n","        if loader.dataset.use_gpu:\n","            vol = vol.cuda()\n","            label = label.cuda()\n","        vol = Variable(vol)\n","        label = Variable(label)\n","\n","        logit = model.forward(vol)\n","\n","        loss = loader.dataset.weighted_loss(logit, label)\n","        total_loss += loss.item()\n","\n","        pred = torch.sigmoid(logit)\n","        pred_npy = pred.data.cpu().numpy()[0][0]\n","        label_npy = label.data.cpu().numpy()[0][0]\n","\n","        preds.append(pred_npy)\n","        labels.append(label_npy)\n","\n","        if train:\n","            loss.backward()\n","            optimizer.step()\n","        num_batches += 1\n","\n","    avg_loss = total_loss / num_batches\n","\n","    fpr, tpr, threshold = metrics.roc_curve(labels, preds)\n","    auc = metrics.auc(fpr, tpr)\n","\n","    return avg_loss, auc, preds, labels\n","\n","def evaluate(split, model_path, diagnosis, use_gpu):\n","    train_loader, valid_loader, test_loader = load_data(diagnosis, use_gpu)\n","\n","    model = MRNet()\n","    state_dict = torch.load(model_path, map_location=(None if use_gpu else 'cpu'))\n","    model.load_state_dict(state_dict)\n","\n","    if use_gpu:\n","        model = model.cuda()\n","\n","    if split == 'train':\n","        loader = train_loader\n","    elif split == 'valid':\n","        loader = valid_loader\n","    elif split == 'test':\n","        loader = test_loader\n","    else:\n","        raise ValueError(\"split must be 'train', 'valid', or 'test'\")\n","\n","    loss, auc, preds, labels = run_model(model, loader)\n","\n","    print(f'{split} loss: {loss:0.4f}')\n","    print(f'{split} AUC: {auc:0.4f}')\n","\n","    return preds, labels\n","\n","#if __name__ == '__main__':\n","#    args = get_parser().parse_args()\n","#   evaluate(args.split, args.model_path, args.diagnosis, args.gpu)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5_S6k0IDwVi","colab_type":"code","colab":{}},"source":["# train.py\n","\n","import argparse\n","import json\n","import numpy as np\n","import os\n","import torch\n","\n","from datetime import datetime\n","from pathlib import Path\n","from sklearn import metrics\n","\n","#from evaluate import run_model\n","#from loader import load_data\n","#from model import MRNet\n","\n","def train(rundir, diagnosis, epochs, learning_rate, use_gpu):\n","    val_auc_array = list()\n","    train_auc_array = list()\n","    train_loader, valid_loader, test_loader = load_data(diagnosis, use_gpu)\n","    \n","    model = MRNet()\n","    \n","    if use_gpu:\n","        model = model.cuda()\n","\n","    # modify the code in this\n","    # try with RAdam\n","    # grid search?\n","    # can try without weight decay\n","    optimizer = torch.optim.Adam(model.parameters(), learning_rate, weight_decay=.01)\n","\n","    # patience too low (after 5 epochs, if AUC hasnt improved, slash learning rate .3), which is why high learning rate seems to work better\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=.3, threshold=1e-4)\n","\n","    best_val_loss = float('inf')\n","\n","    start_time = datetime.now()\n","\n","    for epoch in range(epochs):\n","        change = datetime.now() - start_time\n","        print('starting epoch {}. time passed: {}'.format(epoch+1, str(change)))\n","        \n","        train_loss, train_auc, _, _ = run_model(model, train_loader, train=True, optimizer=optimizer)\n","        print(f'train loss: {train_loss:0.4f}')\n","        print(f'train AUC: {train_auc:0.4f}')\n","\n","        val_loss, val_auc, _, _ = run_model(model, valid_loader)\n","        print(f'valid loss: {val_loss:0.4f}')\n","        print(f'valid AUC: {val_auc:0.4f}')\n","        val_auc_array.append(val_auc)\n","        train_auc_array.append(train_auc)\n","        \n","        scheduler.step(val_loss)\n","\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","\n","            file_name = f'val{val_loss:0.4f}_train{train_loss:0.4f}_epoch{epoch+1}'\n","            save_path = Path(rundir) / file_name\n","            # dont need to save stuff for now, model is too shitty\n","            #torch.save(model.state_dict(), save_path)\n","            \n","    return val_auc_array, train_auc_array\n","\n","def get_parser():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--rundir', type=str, required=True)\n","    parser.add_argument('--diagnosis', type=int, required=True)\n","    parser.add_argument('--seed', default=42, type=int)\n","    parser.add_argument('--gpu', action='store_true')\n","    parser.add_argument('--learning_rate', default=1e-05, type=float)\n","    parser.add_argument('--weight_decay', default=0.01, type=float)\n","    parser.add_argument('--epochs', default=50, type=int)\n","    parser.add_argument('--max_patience', default=5, type=int)\n","    parser.add_argument('--factor', default=0.3, type=float)\n","    return parser\n","\n","#if __name__ == '__main__':\n","#    args = get_parser().parse_args()\n","    \n","#    np.random.seed(args.seed)\n","#    torch.manual_seed(args.seed)\n","#    if args.gpu:\n","#        torch.cuda.manual_seed_all(args.seed)\n","\n","#    os.makedirs(args.rundir, exist_ok=True)\n","    \n","#    with open(Path(args.rundir) / 'args.json', 'w') as out:\n","#        json.dump(vars(args), out, indent=4)\n","\n","#    train(args.rundir, args.diagnosis, args.epochs, args.learning_rate, args.gpu)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xDIZFy0IrqVs","colab_type":"code","outputId":"9fe07ff7-49bf-41ae-a0d0-e7f405e776c7","executionInfo":{"status":"error","timestamp":1577376724352,"user_tz":300,"elapsed":155540,"user":{"displayName":"Miles Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCp1D9GSnniuak1b4qTc8LyXIYWr5-w3JOCol8uMDoJ9DjR6Oc2jUCzv2CLHZySNgZQZKBOu9XQQ61wZgOFmS7HnfhhjOjtGNycWsDYEOVN2QXaIyeAFjqs1wpthUa5qoxxSxIBIQwsQRbqshUrkEXMM12r56GOEq2njIfcq6fpRLzNSIP4KBJWU_kv2U3nBqRgN_Z4BKSs6VZeo9k7SOcT1wnSyavpy59kXDXuZkE4b1xBPgAqftswGN9ptbGevL8XW6FnD2y4RElZw5Rn4lZEKAE7zuZfZFtc9EdjF55pCSmgzgG8K6e6w1pPSO-vbRb62xAFhsWuT_CrDp70jm2kPPAZTWJqJfTnfOVimQza6UzgAJeliOI9IVJvVBPxJdoTjyAsfvD-wt949h1dKESssk_kq9PgCC3wWpc8P5-UJt7W_umXtj9lk0H6oIgS9h6UhEmyNSmDc0hPnaShr8GiwNwGujjlRhoxG_UTbqbOcPUxNnHMtBvz0q4jSaATG2_scmW3KHGrm9RrE4xttEX0eee1Q-0FPvkLM5oTyBniAeZmmNZZ1JybeEfGFAmD3Zvq4tFXvDuENVKVRRdU3AZx2pt6fUh1zApSbDly5AfTvPHqX3Wfr9e7aCqo2ZKEmEWpS0um7w0uqInV8LtF1_i33iUOMdxkIdh_HpVT9-g8u5buBGiWLvbx2PJ4YcOW9hVZvIABh0mFZUTeVg9zIFHWYa037siwMhH2MExVM6MKkYfepaH5utn5JhaVbdw=s64","userId":"12217843719772555429"}},"colab":{"base_uri":"https://localhost:8080/","height":622}},"source":[" import os\n","%cd \"/content/gdrive/My Drive/thesis/Data/vols\"\n","\n","#data path\n","train_path = \"/content/gdrive/My Drive/thesis/Data/train\"\n","data_path = \"/content/gdrive/My Drive/thesis/Data/vols\"\n","#train_meniscus_csv_path = data_path + '/train-meniscus.csv'\n","\n","gpu = True\n","seed = 42\n","np.random.seed(seed)\n","torch.manual_seed(seed)\n","\n","\n","#learningrate = 5e-05\n","epochs = 50\n","diagnosis = 0\n","rundir = data_path\n","\n","if gpu:\n","  torch.cuda.manual_seed_all(seed)\n","\n","# larger learning rates\n","learn1 = 5e-05\n","learn2 = 1e-04\n","learn3 = 5e-04\n","learn4 = 1e-03\n","learn5 = 2e-03\n","\n","varray1, tarray1 = train(rundir, diagnosis, epochs, learn1, gpu)\n","varray2, tarray2 = train(rundir, diagnosis, epochs, learn2, gpu)\n","varray3, tarray3 = train(rundir, diagnosis, epochs, learn3, gpu)\n","varray4, tarray4 = train(rundir, diagnosis, epochs, learn4, gpu)\n","varray5, tarray5 = train(rundir, diagnosis, epochs, learn5, gpu)\n","\n","#varray2, tarray2 = varray1, tarray1\n","#varray3, tarray3 = varray1, tarray1\n","#varray4, tarray4 = varray1, tarray1\n","#varray5, tarray5 = varray1, tarray1\n","\n","def display_line(x_length, lr1, y1, lr2, y2, lr3, y3, lr4, y4, lr5, y5, title, xlabel, ylabel):\n","  plt.figure(0)\n","  # see if we can set axis later\n","  #ax = plt.axis()\n","  #ax.set(xlim = (np.min(x),np.max(x)), option='tight')\n","  plt.title(title)\n","  plt.plot(np.arange(x_length), y1, label=str(lr1))\n","  plt.plot(np.arange(x_length), y2, label=str(lr2))\n","  plt.plot(np.arange(x_length), y3, label=str(lr3))\n","  plt.plot(np.arange(x_length), y4, label=str(lr4))\n","  plt.plot(np.arange(x_length), y5, label=str(lr5))\n","  plt.legend()\n","  plt.xlabel(xlabel)\n","  plt.ylabel(ylabel)\n","  plt.show()\n","  return\n","\n","display_line(epochs, learn1, varray1, learn2, varray2, learn3, varray3, learn4, varray4, learn5, varray5, \"Trained AlexNet Val AUC over epochs diagnosis = \" + str(diagnosis), \"epoch\", \"Validation AUC\")\n","display_line(epochs, learn1, tarray1, learn2, tarray2, learn3, tarray3, learn4, tarray4, learn5, tarray5, \"Trained AlexNet Train AUC over epochs diagnosis = \" + str(diagnosis), \"epoch\", \"Training AUC\")"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/thesis/Data/vols\n","['vol08', 'vol04', 'vol03', 'vol09', 'vol06', 'vol07']\n","['vol10', 'vol05']\n","['vol01', 'vol02']\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\" to /root/.cache/torch/checkpoints/alexnet-owt-4df8aa71.pth\n","100%|██████████| 233M/233M [00:22<00:00, 10.8MB/s]\n"],"name":"stderr"},{"output_type":"stream","text":["starting epoch 1. time passed: 0:00:00.000010\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-b62bfb285d83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mlearn5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-03\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mvarray1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarray1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrundir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagnosis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mvarray2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarray2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrundir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagnosis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mvarray3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarray3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrundir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiagnosis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-f797ba3b1b82>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(rundir, diagnosis, epochs, learning_rate, use_gpu)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'starting epoch {}. time passed: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_auc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train loss: {train_loss:0.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'train AUC: {train_auc:0.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-b21009a52346>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model, loader, train, optimizer)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    817\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    844\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mAssertionError\u001b[0m: Caught AssertionError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-1-8cbc564f0ea7>\", line 210, in __getitem__\n    assert(1==3)\nAssertionError\n"]}]},{"cell_type":"code","metadata":{"id":"_WkAra1-5PfZ","colab_type":"code","colab":{}},"source":["\n","#evaluate('test', 'val0.0175_train0.0139_epoch50', 1, True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vX6gABy_8OZ0","colab_type":"code","colab":{}},"source":["\n","def display1_line(x_length, lr1, y1, lr2, y2, lr3, y3, lr4, y4, lr5, y5, title, xlabel, ylabel):\n","  plt.figure(0)\n","  # see if we can set axis later\n","  #ax = plt.axis()\n","  #ax.set(xlim = (np.min(x),np.max(x)), option='tight')\n","  plt.title(title)\n","  plt.plot(np.arange(x_length), y1, label=str(lr1))\n","  plt.plot(np.arange(x_length), y2, label=str(lr2))\n","  plt.plot(np.arange(x_length), y3, label=str(lr3))\n","  plt.plot(np.arange(x_length), y4, label=str(lr4))\n","  plt.plot(np.arange(x_length), y5, label=str(lr5))\n","  plt.legend()\n","  plt.xlabel(xlabel)\n","  plt.ylabel(ylabel)\n","  plt.show()\n","  return\n","\n","display1_line(epochs, 5e-06, varray1, 1e-05, varray2, 5e-05, varray3, 1e-04, varray4, 1e-03, varray5, \"AUC loss over epochs diagnosis = \" + str(diagnosis), \"epoch\", \"Validation AUC\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bg-KWGHW8Wlo","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJPhauQm8X4F","colab_type":"code","colab":{}},"source":["# notes from ben\n","# get rid of double standardization, dont have RGB channels all have same information, make AlexNet take 1 channel instead of 3. \n","# try to dissect images into two camps, 3.0T and 1.5T.\n","# Then we can try to change into DenseNets. "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4L4xN-i77gB2","colab_type":"code","colab":{}},"source":["p-values[rot, crop, shift] = [0.3, 0.4, 0.5]\n","\n","for every image in folder:\n","  load image\n","\n","  if random number generator < p-values rot:\n","    image = rotate image (by x degrees)\n","  if random number generator < p-values crop:\n","    image = crop image (by x,y values)\n","  if random number generator < p-values shift:\n","    image = shift image (by x values in y direction)\n","\n","  convert image into tensor for processing\n","  send image to training batch, grab next image"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rEu3cRZK7fgG","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"6o5ou_z-8axx","colab_type":"code","colab":{}},"source":["define initial state of model\n","(Lets say a simple model of 3 layers, with ReLU activation functions and\n"," down-sampling method of max pool. Input image is 256x256)\n","features = Sequential model: \n","  Layer: Convolutional 2 dimensional (256 input, 128 output, padding = 2)\n","  Activation Function: ReLU\n","  Down-Sample: maxpool\n","  Layer: Convolutional 2 dimensional (128 input, 64 output, padding = 2)\n","  Activation Function: ReLU\n","  Down-Sample: maxpool\n","  Layer: Convolutional 2 dimensional (64 input, 32 output, padding = 2)\n","  Activation Function: ReLU\n","  Down-Sample: maxpool\n","\n","  Layer: Softmax (to convert a 32x32 matrix to probability distribution used to classify)\n"],"execution_count":0,"outputs":[]}]}